{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Rv18UaRljJX"
   },
   "source": [
    "## CPSC 436N Assignment 1 - Language Models\n",
    "\n",
    "In this assignment you will implement both a count-based N-gram language model and a simple neural language model.\n",
    "\n",
    "For the neural model, familiarize yourself with pytorch: https://pytorch.org/tutorials/beginner/basics/intro.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4otuXYsljJb"
   },
   "source": [
    "\n",
    "\n",
    "Let's load a number of dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "7t9hPaLcljJd"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import count\n",
    "from scipy.special import softmax\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Check if GPU is available to pytorch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# needed for working in log space\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhWxwA6JljJf"
   },
   "source": [
    "Next, we implement the corpus and vocabulary objects. The vocabulary object has an index for each word type, and the corpus object reads a text corpus and returns the word type index for each word instance in the corpus. It is a list of lines.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "--IW-bTprKHJ"
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = defaultdict(count(0).__next__)\n",
    "\n",
    "        # Add special tokens\n",
    "        _ = self.word2idx['<start>']\n",
    "        _ = self.word2idx['<end>']\n",
    "        _ = self.word2idx['<unk>']\n",
    "\n",
    "    def add_word(self, word):\n",
    "        _ = self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "    def idx2word(self):\n",
    "        return {i: w for w, i in self.word2idx.items()}\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path, n=2, vocab=None):\n",
    "        # Only initialize for the train corpus.\n",
    "        # Then, for the dev and test corpus, use the vocabulary\n",
    "        # from the training set\n",
    "        if vocab is None:\n",
    "          self.vocab = Vocabulary()\n",
    "        else:\n",
    "          self.vocab = vocab\n",
    "\n",
    "        # Read the entire corpus to memory\n",
    "        lines = open(path).readlines()\n",
    "\n",
    "        # \"Tokenize\" (split to words) and add the <start> tokens based on the\n",
    "        # number of n-grams, and the <end> tokens, for each line.\n",
    "        # We will lowercase the corpus because we don't have enough data.\n",
    "        lines = [['<start>'] * (n-1) + l.lower().split() + ['<end>'] for l in lines]\n",
    "\n",
    "        # Convert word instance to word type ID. If we are building the\n",
    "        # vocabulary, add new words, other map to existing words or '<unk>'.\n",
    "        if vocab is None:\n",
    "\n",
    "          # Only keep words that appeared at least twice in the corpus\n",
    "          # and use <unk> for infrequent words.\n",
    "          freq = dict(Counter([w for line in lines for w in line]))\n",
    "          lines = [[self.vocab.word2idx[w]\n",
    "                    if freq[w] > 1\n",
    "                    else self.vocab.word2idx['<unk>']\n",
    "                    for w in l] for l in lines]\n",
    "\n",
    "          # Convert from defaultdict to dictionary so that we can't add more tokens\n",
    "          self.vocab.word2idx = dict(self.vocab.word2idx)\n",
    "        else:\n",
    "          lines = [[self.vocab.word2idx.get(w, self.vocab.word2idx['<unk>'])\n",
    "                    for w in l]\n",
    "                   for l in lines]\n",
    "\n",
    "        self.lines = lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMz414GKS2hO"
   },
   "source": [
    "Load the train, dev, and test sets. These are samples from Simple Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wmZBKmqjljJl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train number of sentences: 25000, vocab size: 25692\n",
      "dev number of sentences: 5000, vocab size: 25692\n",
      "test number of sentences: 5000, vocab size: 25692\n"
     ]
    }
   ],
   "source": [
    "train = Corpus(\"data/train.txt\")\n",
    "dev = Corpus(\"data/dev.txt\", vocab=train.vocab)\n",
    "test = Corpus(\"data/test.txt\", vocab=train.vocab)\n",
    "\n",
    "for s, name in zip([train, dev, test], [\"train\", \"dev\", \"test\"]):\n",
    "  print(f\"{name} number of sentences: {len(s.lines)}, vocab size: {len(s.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSgcuuCAljJm"
   },
   "source": [
    "### Bigram (Count-based) Language Model\n",
    "\n",
    "We will create a `CountBasedLM` object. Its training includes computing all the ngram counts in the corpus, for `n` and `n-1`. For inference you will need to complete the implementation of `compute_mle`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "iuKg7_wqljJm"
   },
   "outputs": [],
   "source": [
    "def get_ngrams(s, n):\n",
    "  \"\"\"\n",
    "  Gets a list of words and returns a list of n-grams\n",
    "  \"\"\"\n",
    "  return zip(*[s[i:] for i in range(n)])\n",
    "\n",
    "\n",
    "class CountBasedLM(object):\n",
    "    def __init__(self, n=2, laplace=1):\n",
    "        self.vocab = []\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.laplace = laplace\n",
    "        self.n = n\n",
    "\n",
    "    def train(self, train_corpus):\n",
    "        self.vocab = train_corpus.vocab\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "        # Count the n-grams in the corpus\n",
    "        words = [w for line in train_corpus.lines for w in line]\n",
    "\n",
    "        self.n_gram_count = dict(Counter(get_ngrams(words, self.n)))\n",
    "        self.nm1_gram_count = dict(Counter(get_ngrams(words, self.n-1)))\n",
    "\n",
    "    def compute_mle(self, n_gram):\n",
    "        \"\"\"\n",
    "        Compute the MLE of P(w_i|w_{i-n+1}...w_{iâˆ’1}),\n",
    "        with add-one Laplacian smoothing.\n",
    "        Please see chapter 3.5.1 of J&M 3rd Ed. for more information.\n",
    "        \"\"\"\n",
    "        ####################################\n",
    "        #   Your code here\n",
    "        ####################################\n",
    "        # Add-one Laplace smoothing: add one to numerator\n",
    "        counts_n = self.n_gram_count.get(n_gram, 0) + 1       \n",
    "\n",
    "        # get the n-1 gram of the current n-gram\n",
    "        nm1_gram = n_gram[:-1]\n",
    "        counts_nm1 = self.nm1_gram_count.get(nm1_gram, 0) + self.vocab_size\n",
    "        \n",
    "        prob = counts_n / counts_nm1\n",
    "        ####################################\n",
    "\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lixRBxehljJy"
   },
   "source": [
    "Let's create and train the bigram LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8YFIiHH-ljJy"
   },
   "outputs": [],
   "source": [
    "bigram_lm = CountBasedLM()\n",
    "bigram_lm.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7uEzSEmqBTmh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003803872342044201"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################################\n",
    "#   Answer to Q1\n",
    "####################################\n",
    "token_ids = tuple([bigram_lm.vocab.word2idx[t] for t in [\"national\", \"park\"]])\n",
    "bigram_lm.compute_mle(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-NrVBqzpRyh"
   },
   "source": [
    "Now, you will implement a function that uses the LM to compute the probability of a sentence based on the chain rule. There are several important implementation details:\n",
    "\n",
    "1. The function gets a (string) sentence so the first step should be to tokenize it and convert the tokens to token IDs.\n",
    "\n",
    "2. Work in log space to avoid underflow, but return the probability (not log probability).\n",
    "\n",
    "3. Do not normalize the probability by sentence length.\n",
    "\n",
    "We will test this function by making sure that:\n",
    "\n",
    "* The probability of each sentence is between 0 and 1\n",
    "\n",
    "* The probability of a sentence is not higher when a word is added\n",
    "\n",
    "* The probability of a grammatical sentence is higher than that of an ungrammatical sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "2ISvoVuwpRi5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.549747446191864e-08\n",
      "2.4089650583948504e-12\n",
      "3.694731684654674e-15\n"
     ]
    }
   ],
   "source": [
    "def compute_probability(lm, sentence):\n",
    "    ####################################\n",
    "    #   Your code here\n",
    "    ####################################\n",
    "    # Split sentence into tokens, convert tokens to token IDs\n",
    "    tokens = sentence.split()\n",
    "    token_ids = tuple([bigram_lm.vocab.word2idx[t] for t in tokens])\n",
    "\n",
    "    # Break tokens into overlapping n-grams\n",
    "    token_ngrams = get_ngrams(token_ids, lm.n)\n",
    "\n",
    "    # Compute the log-probability using the chain rule\n",
    "    log_prob = 0.0\n",
    "    for ngram in token_ngrams:\n",
    "        ngram_prob = lm.compute_mle(ngram)\n",
    "        log_prob += math.log(ngram_prob)    \n",
    "\n",
    "    # Convert back from log to raw probability\n",
    "    probability = math.exp(log_prob)\n",
    "    ####################################    \n",
    "    return probability\n",
    "\n",
    "\n",
    "s1 = \"this is a sentence\"\n",
    "prob_s1 = compute_probability(bigram_lm, s1)\n",
    "assert(0 <= prob_s1 <= 1)\n",
    "\n",
    "s2 = \"this is a longer sentence\"\n",
    "prob_s2 = compute_probability(bigram_lm, s2)\n",
    "assert(prob_s2 <= prob_s1)\n",
    "\n",
    "s3 = \"this longer is a sentence\"\n",
    "prob_s3 = compute_probability(bigram_lm, s3)\n",
    "assert(prob_s3 < prob_s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUZcxvwuVSJC"
   },
   "source": [
    "Now, complete the code for computing average perplexity on a dataset using the LM. We will then compute the perplexity on each of the training, dev, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmsntk-6VYBV"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def compute_perplexity(lm, corpus):\n",
    "  ####################################\n",
    "  #   Your code here\n",
    "  ####################################\n",
    "\n",
    "  ####################################\n",
    "  return perplexity\n",
    "\n",
    "\n",
    "for s, name in zip([train, dev, test], [\"train\", \"dev\", \"test\"]):\n",
    "  ppl = compute_perplexity(bigram_lm, s)\n",
    "  print(f\"The average {name} perplexity for the bigram LM: {ppl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVXSQzPDwXTq"
   },
   "source": [
    "Finally, let's train a trigram model and compare their perplexities on the dev set (remember, lower perplexity is better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSUjRKSOwXCO"
   },
   "outputs": [],
   "source": [
    "# Reload the corpus for the trigram model (we need to add 2 special tokens to\n",
    "# mark the beginning of the sentence)\n",
    "trigram_train = Corpus(\"data/train.txt\", n=3)\n",
    "trigram_dev = Corpus(\"data/dev.txt\", vocab=train.vocab, n=3)\n",
    "trigram_test = Corpus(\"data/test.txt\", vocab=train.vocab, n=3)\n",
    "\n",
    "trigram_lm = CountBasedLM(n=3)\n",
    "trigram_lm.train(trigram_train)\n",
    "\n",
    "for s, name in zip([trigram_train, trigram_dev, trigram_test], [\"train\", \"dev\", \"test\"]):\n",
    "  ppl = compute_perplexity(trigram_lm, s)\n",
    "  print(f\"The average {name} perplexity for the trigram LM: {ppl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGuzE_TXVYPO"
   },
   "source": [
    "We can now use the count-based LM to generate text. This is done by starting with an input `prompt`, computing the distribution for the next token, and sampling from it. We will implement `top k` decoding that prunes the distribution to the k most probable next tokens, re-normalizes it and samples from it proportionally to the probability for each token. Using `k=1` would enable greedy decoding, i.e. selecting the most probable next token at each time step. We will generate the text once with greedy decoding and 5 times with `k=1000`. Complete the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n95DYxr9GBDb"
   },
   "outputs": [],
   "source": [
    "def generate_text_count_based(lm, prompt, k=1, max_tokens=10):\n",
    "    # Tokenize and convert the sentence to a list of IDs\n",
    "    tokens = ['<start>'] * (lm.n-1) + prompt.split()\n",
    "    input_tokens = [lm.vocab.word2idx[t] for t in tokens]\n",
    "\n",
    "    # Iteratively generate the next word until generating <end>\n",
    "    # or until reaching max_tokens.\n",
    "    generated_tokens = []\n",
    "\n",
    "    for i in range(max_tokens):\n",
    "      ####################################\n",
    "      #   Your code here\n",
    "      ####################################\n",
    "\n",
    "      ####################################\n",
    "\n",
    "      generated_tokens.append(selected_index)\n",
    "\n",
    "      if lm.vocab.idx2word()[selected_index] == \"<end>\":\n",
    "        break\n",
    "\n",
    "    return \" \".join([lm.vocab.idx2word()[i] for i in input_tokens + generated_tokens])\n",
    "\n",
    "\n",
    "print(\"k=1\")\n",
    "print(generate_text_count_based(bigram_lm, \"he was born\", k=1, max_tokens=10))\n",
    "\n",
    "print(\"k=100\")\n",
    "for _ in range(5):\n",
    "  print(generate_text_count_based(bigram_lm, \"he was born\", k=100, max_tokens=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p10QIUn9qasE"
   },
   "source": [
    "### Neural Language Model\n",
    "\n",
    "The neural LM is an Ngram LM with one hidden layer as we learned in class. Let's start by defining some hyperparameters. Feel free to change them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99hAuwZaqasE"
   },
   "outputs": [],
   "source": [
    "embed_size = 64\n",
    "hidden_size = 128\n",
    "num_epochs = 2\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cl7DhAIljJ0"
   },
   "source": [
    "Now, you will implement the class for the neural ngram language model. You will complete the `forward` function that gets a tensor with the context (n-1) token IDs, and returns the unnormalized next token probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZi51vWzljJ1"
   },
   "outputs": [],
   "source": [
    "class NeuralNgramLM(nn.Module):\n",
    "    def __init__(self, vocab, embed_size, intermediate_size, n=2):\n",
    "        super(NeuralNgramLM, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.n = n\n",
    "        self.embed = nn.Embedding(self.vocab_size, embed_size)\n",
    "        self.hidden = nn.Linear((self.n-1) * embed_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, self.vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ####################################\n",
    "        #   Your code here\n",
    "        ####################################\n",
    "\n",
    "        ####################################\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFfNEeZrljJ1"
   },
   "source": [
    "Next, let's create and train a neural bigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gK_WyjiOljJ1"
   },
   "outputs": [],
   "source": [
    "neural_lm = NeuralNgramLM(train.vocab, embed_size, hidden_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(\n",
    "    neural_lm.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jb-yeBUKljJ2"
   },
   "outputs": [],
   "source": [
    "train_ids = [torch.from_numpy(np.array(line)).unsqueeze(0) for line in train.lines]\n",
    "dev_ids = [torch.from_numpy(np.array(line)).unsqueeze(0) for line in dev.lines]\n",
    "\n",
    "neural_lm.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for instance in tqdm(train_ids):\n",
    "        # Notice that we get a [batch_size, max_len-1] tensor\n",
    "        # which would only work for a bigram LM.\n",
    "        # If we want to train a n > 2 LM, we would need\n",
    "        # add a dimension for the context.\n",
    "        inputs = instance[:, :-1].to(device)\n",
    "        targets = instance[:, 1:].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        # (the outputs shape should be [batch_size, max_len, vocab_size])\n",
    "        outputs = neural_lm(inputs)\n",
    "        loss = criterion(\n",
    "            outputs.reshape(-1, neural_lm.vocab_size), targets.reshape(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate the perplexity of the current trained model on the dev set.\n",
    "    # It should reduce between epochs. We will not use our implementation of\n",
    "    # perplexity but instead we will use cross-entropy loss. See J&M for the\n",
    "    # relationship between the two.\n",
    "    total_ppl = 0\n",
    "    for i in range(0, len(dev_ids)):\n",
    "        dev_inputs = dev_ids[i][:, :-1].to(device)\n",
    "        dev_targets = dev_ids[i][:,1:].to(device)\n",
    "        dev_outputs = neural_lm(dev_inputs)\n",
    "        ce = criterion(\n",
    "            dev_outputs.reshape(-1, neural_lm.vocab_size),\n",
    "            dev_targets.reshape(-1))\n",
    "        total_ppl += ce.item()\n",
    "\n",
    "    print(f\"\\nEpoch [{epoch + 1}/{num_epochs}], \" + \\\n",
    "          f\"Training Loss: {total_loss/len(train_ids):.4f}, \" + \\\n",
    "          f\"Dev Perplexity: {np.exp(total_ppl/len(dev_ids)):5.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCbcvvDwljJ2"
   },
   "source": [
    "Save the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WEU68Mc0ljJ3"
   },
   "outputs": [],
   "source": [
    "torch.save(neural_lm, f\"neural_{neural_lm.n}gram_lm.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRvZAdgSljJ3"
   },
   "source": [
    "Finally, let's load the model and test it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "UKu9d0lLljJ3",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "test_ids = [torch.from_numpy(np.array(line)).unsqueeze(0) for line in test.lines]\n",
    "\n",
    "neural_model = torch.load(f\"neural_{neural_lm.n}gram_lm.ckpt\", weights_only=False)\n",
    "neural_model.eval()\n",
    "test_ppl = 0\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(test_ids)):\n",
    "      inputs = test_ids[i][:, :-1].to(device)\n",
    "      targets = test_ids[i][:,1:].to(device)\n",
    "      outputs = neural_lm(inputs)\n",
    "      ce = criterion(outputs.reshape(-1, neural_lm.vocab_size),\n",
    "                     targets.reshape(-1))\n",
    "      test_ppl += ce.item()\n",
    "\n",
    "print(f\"Test Perplexity: {np.exp(test_ppl/len(test_ids)):5.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIQEXb6vTyul"
   },
   "source": [
    "We can compute the probability of sentences using the neural LM. In fact, cross entropy loss between the first `|S|-1` tokens and the `|S|-1` last tokens is the negative log likelihood of a sequence, i.e. it is -1 * the average of log probabilities for each token. Thanks to the monotonicity of log function, lower cross entropy loss corresponds to higer probability for a sentence. Since we are typically interested in comparing probabilities of sentences (rather than the absolute value), we can use CE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhNuNs-IT8Pr"
   },
   "outputs": [],
   "source": [
    "def compute_negative_log_likelihood(neural_lm, sentence):\n",
    "  ####################################\n",
    "  #   Your code here\n",
    "  ####################################\n",
    "\n",
    "  ####################################\n",
    "\n",
    "s = \"this is a park\"\n",
    "prob_s = -compute_negative_log_likelihood(neural_lm, s1)\n",
    "print(prob_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnyiaeVc3SS9"
   },
   "source": [
    "We can also use the neural LM to generate text. Complete the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukTvd6oV5bAh"
   },
   "outputs": [],
   "source": [
    "def generate_text_neural(neural_lm, prompt, k=1, max_tokens=10):\n",
    "    neural_lm.eval()\n",
    "\n",
    "    # Tokenize and convert the sentence to a list of IDs\n",
    "    tokens = ['<start>'] * (neural_lm.n-1) + prompt.split()\n",
    "    input_tokens = [neural_lm.vocab.word2idx.get(t, neural_lm.vocab.word2idx['<unk>']) for t in tokens]\n",
    "\n",
    "    # Iteratively generate the next word until generating <end>\n",
    "    # or until reaching max_tokens.\n",
    "    generated_tokens = []\n",
    "\n",
    "    for i in range(max_tokens):\n",
    "      token_ids = torch.tensor(input_tokens + generated_tokens, device=device).unsqueeze(0)\n",
    "\n",
    "      ####################################\n",
    "      #   Your code here\n",
    "      ####################################\n",
    "\n",
    "\n",
    "      ####################################\n",
    "\n",
    "      generated_tokens.append(selected_index)\n",
    "\n",
    "      if neural_lm.vocab.idx2word()[selected_index] == \"<end>\":\n",
    "        break\n",
    "\n",
    "    return \" \".join([neural_lm.vocab.idx2word()[i]\n",
    "                     for i in input_tokens + generated_tokens])\n",
    "\n",
    "\n",
    "print(\"k=1\")\n",
    "print(generate_text_neural(neural_lm, \"he was born\", k=1, max_tokens=10))\n",
    "\n",
    "print(\"k=100\")\n",
    "for _ in range(5):\n",
    "  print(generate_text_neural(neural_lm, \"he was born\", k=10, max_tokens=10))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (436_env)",
   "language": "python",
   "name": "436_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
